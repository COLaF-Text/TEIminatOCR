<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>TEIminatOCR, an image to TEI full javascript application</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    <script src='https://cdn.jsdelivr.net/npm/tesseract.js@5/dist/tesseract.min.js'></script>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous">
    <style>
      canvas {
          display:block;
          border: 1px solid black;
          margin-top:10px;
      }
    </style>
</head>
<body>
    <div class="container-fluid">
      <div class="row">
        <div class="col-md-10 offset-md-1 mb-2 mt-2">
          <div class="mb-2">
            <h1>Welcome to the TEIminatOCR</h1>
            <p>TEIminatOCR is an application that reuses object identification for segmenting and recognizing zones of texts, OCR using TesseractJS and the Segmonto ontology. Images that you will load will be first segmented through YOLO object detection, then OCRized. Bounding box of lines are then dispatched in each area and the TEI is produced for the output.</p>
            <h3>Bibliography</h3>
            <ul style="font-size:smaller;">
              <li>
                Jocher, G., Chaurasia, A., & Qiu, J. (2023). YOLO by Ultralytics (Version 8.0.0) [Computer software]. <a href="https://github.com/ultralytics/ultralytics">https://github.com/ultralytics/ultralytics</a>
              </li>
              <li>
                Clérice, T. (2023). You Actually Look Twice At it (YALTAi): using an object detection approach instead of region segmentation within the Kraken engine. [Preprint] <a target="_blank" href="https://enc.hal.science/hal-03723208v2">⟨hal-03723208v2⟩</a>
              </li>
              <li>Pinche, A., Gabay, S., & Camps, J. B. (2022). SegmOnto: Vocabulaire contrôlé pour décrire les manuscrits et les imprimés. In Segmenter et annoter les images: déconstruire pour reconstruire. <i>1st International Workshop on Computational Paleography (IWCP@ICDAR 2021)</i>, Lausanne, Switzerland. <a href="https://hal.science/hal-03336528">⟨hal-03336528⟩</a></li>
              <li>R. Smith. (2007). An Overview of the Tesseract OCR Engine. <i>Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)</i>,, vol. 2, pp. 629–633. <a href="https://doi.org/10.1109/ICDAR.2007.4376991">10.1109/ICDAR.2007.4376991</a>.</li>
              <li>
                Wu, J. (2023). TesseractJS (Version 5.0.2) [Computer software]. <a href="https://github.com/naptha/tesseract.js">https://github.com/naptha/tesseract.js</a>
              </li>
            </ul>
          </div>
          <hr />
          <h3>Form</h3>
          <h4>Choose your input</h4>
          <div class="row mb-2">
            <div class="col-sm-3 col-md-2 offset-md-3 offset-sm-3 col-form-label">
              <div class="form-check">
                <input type="radio" name="use" value="uploadInput" class="form-check-input" id="useUpload" />
                <label for="useUpload" class="form-check-label">Upload an image</label>
              </div>
            </div>
            <div class="col-sm-4 text-start">
              <input type="file" class="form-control" id="uploadInput">
            </div>
          </div>
          <div class="row mb-2">
            <div class="col-sm-3 col-md-2 offset-md-3 offset-sm-3 col-form-label">
              <div class="form-check">
                <input type="radio" name="use" value="urlInput" class="form-check-input" id="useURL" checked/>
                <label for="useURL" class="form-check-label">Use an URL</label>
              </div>
            </div>
            <div class="col-sm-4 text-start">
              <input type="url" class="form-control" id="urlInput" value="https://gallica.bnf.fr/iiif/ark:/12148/bd6t5327037p/f582/full/,1200/0/native.jpg">
            </div>
          </div>
          <div class="row mb-2">
            <div class="col-sm-3 col-md-2 offset-md-3 offset-sm-3 col-form-label">
              <div class="form-check">
                <input type="radio" name="use" value="camInput" class="form-check-input" id="useCam"/>
                <label for="useCam" class="form-check-label">Use your phone camera</label>
              </div>
            </div>
            <div class="col-sm-4 text-start">
              <input type="file" class="form-control" id="camInput" accept="image/*" capture="user">
            </div>
          </div>

          <div class="row">
            <div class="col-sm-12 text-center">
              <a class="btn btn-primary" id="button" href="#">Process</a>
            </div>
          </div>
          <hr />
        </div>
      </div>
      <div class="row">
        <div class="col-md-5 offset-md-1">
          <canvas></canvas>
        </div>
        <div class="col-md-5">
          <textarea id="te" style="min-height:400px; width: 100%;"></textarea>
        </div>
      </div>
    </div>

    <script>
      const IMG_WIDTH = 640, 
            IMG_HEIGHT = 640,
            MIN_PROB = .2,
            OVERLAP = 0.3,
            OUT_SIZE = 8400; // Depends on your model (1, 23, 13125) is the output shape for us

      /** Check if an element is in a bbox
       * 
       */
      // Function to check if a point is within a bounding box
      function isPointInBBox(point, bbox) {
          return (
              point[0] >= bbox[0] && // Check X coordinate
              point[0] <= bbox[2] &&
              point[1] >= bbox[1] && // Check Y coordinate
              point[1] <= bbox[3]
          );
      }
      function computeBBoxCenter(bbox) {
          if (bbox.length !== 4) {
              throw new Error("Bounding box should have 4 coordinates: [left, top, right, bottom]");
          }

          const centerX = (bbox[0] + bbox[2]) / 2;
          const centerY = (bbox[1] + bbox[3]) / 2;

          return [centerX, centerY];
      }

      /**
       * "Upload" button onClick handler: uploads selected image file
       * to backend, receives array of detected objects
       * and draws them on top of image
       */
       const textarea = document.getElementById("te"),
             button = document.getElementById("button");

       function getMeTheRadio() {
         const radioButtons = document.getElementsByName("use");
         let selectedValue = null;

          for (let i = 0; i < radioButtons.length; i++) {
            if (radioButtons[i].checked) {
              selectedValue = radioButtons[i].value;
              break; // Exit the loop once the checked radio button is found
            }
          }
         return selectedValue;
       }

       button.addEventListener("click", async(event) => {
          event.preventDefault();
          button.innerText = "Be patient, it's coming";

          // Define input
          const using = getMeTheRadio(),
                input = document.getElementById(using);
          let val = null;

          if (using === "urlInput") {
            val = input.value;
          } else {
            val = input.files[0];
          }

          const out = await detect_objects_on_image(val);
          textarea.value = out.xml.outerHTML.split("<lb>").join("<lb />").replace(/<\/lb>/g, '');
          draw_image_and_boxes(
            val,
            out.boxes,
            out.lines
          );
          button.innerText = "Process";
       })

      /**
       * Function draws the image from provided file
       * and bounding boxes of detected objects on
       * top of the image
       * @param file Uploaded file object
       * @param boxes Array of bounding boxes in format [[x1,y1,x2,y2,object_type,probability],...]
       */
      function draw_image_and_boxes(file, boxes, lines) {
          const img = new Image();
          
          if ((typeof file === 'string' || file instanceof String) && file.startsWith("http")) {
            img.crossOrigin = 'Anonymous';
            img.src = file;
          } else {
            img.src = URL.createObjectURL(file);
          }
          img.onload = () => {
              const canvas = document.querySelector("canvas");
              const scale =  textarea.offsetWidth / img.width;
              canvas.width = textarea.offsetWidth;
              canvas.height = img.height * scale;
              const ctx = canvas.getContext("2d");
              ctx.imageSmoothingEnabled = false;
              ctx.drawImage(img, 0, 0, canvas.width, canvas.height);
              ctx.strokeStyle = "#00FF00";
              ctx.lineWidth = 1;
              ctx.font = "18px serif";
              boxes.forEach(([x1,y1,x2,y2,label]) => {
                  ctx.strokeRect(x1*scale,y1*scale,(x2-x1)*scale,(y2-y1)*scale);
                  ctx.fillStyle = "#00ff00";
                  ctx.fillText(label, x1*scale, (y1+18)*scale);
              });
              lines.forEach(([x1,y1,x2,y2]) => {
                  ctx.strokeRect(x1*scale,y1*scale,(x2-x1)*scale,(y2-y1)*scale);
                  ctx.strokeStyle = "#000000";
              });
          }
      }

      function calculateIntersectionArea(bbox1, bbox2) {
          // Calculate the intersection area between two bounding boxes.
          const x1 = Math.max(bbox1[0], bbox2[0]);
          const y1 = Math.max(bbox1[1], bbox2[1]);
          const x2 = Math.min(bbox1[2], bbox2[2]);
          const y2 = Math.min(bbox1[3], bbox2[3]);

          const width = Math.max(0, x2 - x1);
          const height = Math.max(0, y2 - y1);

          return width * height;
      }

      function getLastObjectKey(obj) {
          const keys = Object.keys(obj);
          return keys[keys.length - 1];
      }


      function dispatchElements(lineList, zoneList) {
          // Create a dictionary to hold the dispatched elements.
          const dispatchDict = {};

          for (const currentLine of lineList) {
              let maxIntersection = 0;
              let targetBbox = null
                  dispatchDictKey = null;

              let currentLineBbox = [
                  currentLine.bbox.x0,
                  currentLine.bbox.y0,
                  currentLine.bbox.x1,
                  currentLine.bbox.y1
                ];

              for (const currentZone of zoneList) {

                  const intersectionArea = calculateIntersectionArea(currentLineBbox, currentZone.slice(0, 4));

                  if (intersectionArea > maxIntersection) {
                      maxIntersection = intersectionArea;
                      targetBbox = currentZone;
                      dispatchDictKey = currentZone.slice(0, 4);
                  }
              }

              if (maxIntersection > 0) {
                  // Add the element to the target bbox in the dictionary.
                  if (!dispatchDict[dispatchDictKey]) {
                      dispatchDict[dispatchDictKey] = { elements: [], isOriginal: true, metadata: targetBbox};
                  }
                  dispatchDict[dispatchDictKey].elements.push(currentLine);
              } else {
                  // If there is no intersection, create a new element and mark it as original.
                  let lastKey = getLastObjectKey(dispatchDict);
                  if ((lastKey) && !(dispatchDict[lastKey].isOriginal)) {
                    dispatchDict[lastKey].elements.push(currentLine);
                  } else {
                    dispatchDict[currentLineBbox] = { elements: [currentLine], isOriginal: false, metadata: null};
                  }
              }
          }

          return dispatchDict;
      }


      /**
       * Function receives an image, passes it through YOLOv8 neural network
       * and returns an array of detected objects and their bounding boxes
       * @param buf Input image body
       * @returns Array of bounding boxes in format [[x1,y1,x2,y2,object_type,probability],..]
       */
      async function detect_objects_on_image(buf) {
          const [img, input,img_width,img_height] = await prepare_input(buf);
          const output = await run_model(input);
          const lines = await OCRize(img);
          boxes = process_output(output,img_width,img_height);

          boxes.sort((a, b) => {
            // Compare the second floating-point element (index 1) first
            if (a[1] !== b[1]) {
                return a[1] - b[1];
            } else {
                // If the second elements are equal, compare the first element (index 0)
                return a[0] - b[0];
            }
          });

          // Boxes : x, y, x2, y2, type, prob

          const xml = document.createElement("body");


          const dispatched = dispatchElements(lines, boxes);

          for (const zone of Object.values(dispatched)) {
            if (zone.isOriginal) {
              xmlNode = document.createElement(getTag(zone.metadata[4]));
            } else {
              xmlNode = document.createElement("ab");
            }
            for (var i = 0; i < zone.elements.length; i++) {
              xmlNode.innerHTML += "<lb/> " + zone.elements[i].text.trim() + "\n";
            }
            xml.appendChild(xmlNode);
          }

          return {
            "boxes": boxes,
            "xml": xml,
            "lines": lines.map((el) => { 
                  return [
                    el.bbox.x0,
                    el.bbox.y0,
                    el.bbox.x1,
                    el.bbox.y1
                  ];
              })
          };
      }

      function getTag(zone) {
        if (zone.startsWith("MainZone")) {
          if (zone.endsWith("-P")) {
            return "p";
          } else if (zone.endsWith("-Head")) {
            return "head";
          } else if (zone.endsWith("-Lg")) {
            return "lg";
          } else if (zone.endsWith("-Sp")) {
            return "sp";
          } else if (zone.endsWith("-Entry")) {
            return "entryFree";
          } else if (zone.endsWith("-Other")) {
            return "ab";
          }
        } else if (zone.startsWith("MarginTextZone")) {
          return "note";
        } else {
          return "ab";
        }

      }

      /**
       * Function used to convert input image to tensor,
       * required as an input to YOLOv8 object detection
       * network.
       * @param buf Content of uploaded file
       * @returns Array of pixels
       */
      async function prepare_input(buf) {
          return new Promise(resolve => {
              const img = new Image();
              if ((typeof buf === 'string' || buf instanceof String) && buf.startsWith("http")) {
                img.crossOrigin = 'Anonymous';
                img.src = buf;
              } else {
                img.src = URL.createObjectURL(buf);
              }
              img.onload = () => {
                  const [img_width,img_height] = [img.width, img.height]
                  const canvas = document.createElement("canvas");
                  canvas.width = IMG_WIDTH;
                  canvas.height = IMG_HEIGHT;
                  const context = canvas.getContext("2d");
                  context.drawImage(img,0,0,IMG_WIDTH,IMG_HEIGHT);
                  const imgData = context.getImageData(0,0,IMG_WIDTH,IMG_HEIGHT);
                  const pixels = imgData.data;

                  const red = [], green = [], blue = [];
                  for (let index=0; index<pixels.length; index+=4) {
                      red.push(pixels[index]/255.0);
                      green.push(pixels[index+1]/255.0);
                      blue.push(pixels[index+2]/255.0);
                  }
                  const input = [...red, ...green, ...blue];
                  resolve([img, input, img_width, img_height])
              }
          })
      }

      /**
       * Function used to pass provided input tensor to YOLOv8 neural network and return result
       * @param input Input pixels array
       * @returns Raw output of neural network as a flat array of numbers
       */
      async function run_model(input) {
          const model = await ort.InferenceSession.create("model.onnx");
          input = new ort.Tensor(Float32Array.from(input),[1, 3, IMG_HEIGHT, IMG_WIDTH]);
          const outputs = await model.run({images:input});
          return outputs["output0"].data;
      }

      /**
       * Function used to convert RAW output from YOLOv8 to an array of detected objects.
       * Each object contain the bounding box of this object, the type of object and the probability
       * @param output Raw output of YOLOv8 network
       * @param img_width Width of original image
       * @param img_height Height of original image
       * @returns Array of detected objects in a format [[x1,y1,x2,y2,object_type,probability],..]
       */
      function process_output(output, img_width, img_height) {
          let boxes = [];
          for (let index=0;index<OUT_SIZE;index++) {
              const [class_id,prob] = [...Array(80).keys()]
                  .map(col => [col, output[OUT_SIZE*(col+4)+index]])
                  .reduce((accum, item) => item[1]>accum[1] ? item : accum,[0,0]);
              if (prob < MIN_PROB) {
                  continue;
              }
              const label = yolo_classes[class_id];
              const xc = output[index];
              const yc = output[OUT_SIZE+index];
              const w = output[2*OUT_SIZE+index];
              const h = output[3*OUT_SIZE+index];
              const x1 = (xc-w/2)/IMG_WIDTH*img_width;
              const y1 = (yc-h/2)/IMG_HEIGHT*img_height;
              const x2 = (xc+w/2)/IMG_WIDTH*img_width;
              const y2 = (yc+h/2)/IMG_HEIGHT*img_height;
              boxes.push([x1,y1,x2,y2,label,prob]);
          }

          boxes = boxes.sort((box1,box2) => box2[5]-box1[5])
          const result = [];
          while (boxes.length>0) {
              result.push(boxes[0]);
              boxes = boxes.filter(box => iou(boxes[0],box)<OVERLAP);
          }
          return result;
      }

      /**
       * Function calculates "Intersection-over-union" coefficient for specified two boxes
       * https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/.
       * @param box1 First box in format: [x1,y1,x2,y2,object_class,probability]
       * @param box2 Second box in format: [x1,y1,x2,y2,object_class,probability]
       * @returns Intersection over union ratio as a float number
       */
      function iou(box1,box2) {
          return intersection(box1,box2)/union(box1,box2);
      }

      /**
       * Function calculates union area of two boxes.
       *     :param box1: First box in format [x1,y1,x2,y2,object_class,probability]
       *     :param box2: Second box in format [x1,y1,x2,y2,object_class,probability]
       *     :return: Area of the boxes union as a float number
       * @param box1 First box in format [x1,y1,x2,y2,object_class,probability]
       * @param box2 Second box in format [x1,y1,x2,y2,object_class,probability]
       * @returns Area of the boxes union as a float number
       */
      function union(box1,box2) {
          const [box1_x1,box1_y1,box1_x2,box1_y2] = box1;
          const [box2_x1,box2_y1,box2_x2,box2_y2] = box2;
          const box1_area = (box1_x2-box1_x1)*(box1_y2-box1_y1)
          const box2_area = (box2_x2-box2_x1)*(box2_y2-box2_y1)
          return box1_area + box2_area - intersection(box1,box2)
      }

      /**
       * Function calculates intersection area of two boxes
       * @param box1 First box in format [x1,y1,x2,y2,object_class,probability]
       * @param box2 Second box in format [x1,y1,x2,y2,object_class,probability]
       * @returns Area of intersection of the boxes as a float number
       */
      function intersection(box1,box2) {
          const [box1_x1,box1_y1,box1_x2,box1_y2] = box1;
          const [box2_x1,box2_y1,box2_x2,box2_y2] = box2;
          const x1 = Math.max(box1_x1,box2_x1);
          const y1 = Math.max(box1_y1,box2_y1);
          const x2 = Math.min(box1_x2,box2_x2);
          const y2 = Math.min(box1_y2,box2_y2);
          return (x2-x1)*(y2-y1)
      }

      /**
       * Array of YOLOv8 class labels
       */
      const yolo_classes = ["DigitizationArtefactZone", "DropCapitalZone", "GraphicZone", "GraphicZone-Legend", "GraphicZone-Maths", "MainZone-Entry", "MainZone-Head", "MainZone-Lg", "MainZone-Other", "MainZone-P", "MainZone-Sp", "MarginTextZone-Notes", "NumberingZone", "PageTitleZone", "PageTitleZone-Index", "RunningTitleZone", "StampZone", "TableZone", "TableZone-Legend"];

      // Javascript

      async function OCRize(img) {
        const worker = await Tesseract.createWorker('fra+osd');
        const {data : { lines }} = await worker.recognize(
          img//,
          // Works badly...
          //{tessedit_pageseg_mode: '1'}
          );
        await worker.terminate();
        return lines;
      };
    </script>
</body>
</html>
